{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwYPrRuDpFmUMmpaSSCffA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hayaboy/ADP_Practical_Exam/blob/main/flicker%EC%9D%B4%EB%AF%B8%EC%A7%80%EB%8B%A4%EC%9A%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moNauDidx1NV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from flickrapi import FlickrAPI\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Flickr API 키 설정\n",
        "API_KEY = '8f7e6b7d84b94f74045d11aeb8470c0a'\n",
        "API_SECRET = 'ae14a34f851a9b0f'\n",
        "flickr = FlickrAPI(API_KEY, API_SECRET, format='parsed-json')\n",
        "\n",
        "# 이미지 검색 함수\n",
        "def search_images(query, max_results=10):\n",
        "    extras = 'url_o'  # 'url_o'는 원본 이미지 URL을 포함합니다.\n",
        "    photos = flickr.photos.search(text=query, per_page=max_results, extras=extras)\n",
        "    return photos['photos']['photo']\n",
        "\n",
        "# 이미지 다운로드 함수\n",
        "def download_image(url, save_path):\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    image.save(save_path)\n",
        "\n",
        "# 검색어에 맞는 이미지 크롤링\n",
        "def crawl_images(query, max_results=10, save_dir='./images'):\n",
        "    photos = search_images(query, max_results)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for i, photo in enumerate(photos):\n",
        "        if 'url_o' in photo:\n",
        "            url = photo['url_o']\n",
        "            save_path = os.path.join(save_dir, f'{query}_{i}.jpg')\n",
        "            download_image(url, save_path)\n",
        "            print(f'Downloaded {save_path}')\n",
        "        else:\n",
        "            print(f'No URL found for photo {photo[\"id\"]}')\n",
        "\n",
        "# 예제 실행\n",
        "if __name__ == '__main__':\n",
        "    query = 'bird'\n",
        "    crawl_images(query, max_results=15, save_dir='./sunset_images')"
      ]
    }
  ]
}